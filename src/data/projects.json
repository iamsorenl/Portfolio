[
    {
        "title": "POS Tagging with Hidden Markov Models and Viterbi Algorithm",
        "startDate": "November 2024",
        "endDate": "December 2024",
        "link": "https://github.com/iamsorenl/HMM-POS-Tagger",
        "muiIcon": "Abc",
        "description": "- Developed a POS tagging system using a Hidden Markov Model (HMM) and implemented the Viterbi algorithm for sequence decoding.\n- Designed and implemented log-space computations for numerical stability, addressing challenges like data sparsity and unseen words using fallback probabilities.\n- Leveraged dynamic programming for efficient computation and implemented structured backpointer mechanisms for accurate sequence reconstruction.\n- Evaluated the system using token-level accuracy and F1-scores to ensure reliability and effectiveness.",
        "tools": [
            "Python",
            "Log-Space Programming",
            "Dynamic Programming",
            "HMM",
            "Viterbi Algorithm"
        ]
    },
    {
        "title": "Airline Sentiment Preprocessing and Modeling",
        "startDate": "November 2024",
        "endDate": "November 2024",
        "link": "https://github.com/iamsorenl/Airline-Sentiment-Preprocessing-and-Modeling",
        "muiIcon": "Abc",
        "description": "- Developed a pipeline for sentiment analysis on the Twitter US Airline Sentiment dataset, focusing on data preprocessing, custom tokenization, and machine learning classification.\n- Applied advanced text cleaning techniques, including regex-based tokenization, lemmatization, and emoji normalization.\n- Trained a support vector machine (SVM) with TF-IDF features, achieving 80.89% test accuracy through preprocessing optimizations and ablation studies.\n- Delivered actionable insights into sentiment trends across airlines and user behavior using visualizations.",
        "tools": [
            "Python",
            "Scikit-learn",
            "Pandas",
            "Regex",
            "Matplotlib"
        ]
    },
    {
        "title": "Building and Evaluating N-Gram Language Models",
        "startDate": "November 2024",
        "endDate": "November 2024",
        "link": "https://github.com/iamsorenl/ngram_project",
        "muiIcon": "Abc",
        "description": "- Built N-Gram models (Unigram, Bigram, Trigram) and Interpolated N-Gram models to predict word sequences and measure perplexity on a subset of the One Billion Word Language Modeling Benchmark dataset.\n- Implemented linear interpolation smoothing and tunable OOV handling to address unseen data and improve model generalization.\n- Optimized interpolation weights (λ1, λ2, λ3) to enhance contextual predictions and reduce perplexity.\n- Analyzed model performance across datasets and demonstrated the effectiveness of interpolation for handling sparse data.",
        "tools": [
            "Python",
            "Numpy",
            "Custom N-Gram Modeling",
            "Perplexity Analysis"
        ]
    },
    {
        "title": "Feature Engineering for Multiclass Classification",
        "startDate": "October 2024",
        "endDate": "November 2024",
        "link": "https://github.com/iamsorenl/feature_engineering_multiclass_classification_SVM_LR_DT",
        "muiIcon": "Abc",
        "description": "- Developed a text classification pipeline for an e-commerce dataset, leveraging feature engineering techniques and multi-class classification models such as SVM, Logistic Regression, and Decision Tree.\n- Implemented GloVe embeddings, Sublinear TF-IDF, and Bag-of-Words features to optimize model performance.\n- Conducted hyperparameter tuning and evaluated performance using metrics like macro-average F1-scores, confusion matrices, and ROC curves.\n- Delivered models with high accuracy and efficient feature representations for text data.",
        "tools": [
            "Python",
            "Scikit-learn",
            "Pandas",
            "GloVe Embeddings",
            "TF-IDF"
        ]
    },
    {
        "title": "Slot Tagging of Natural Language Utterances",
        "startDate": "November 2024",
        "endDate": "November 2024",
        "link": "https://github.com/iamsorenl/Slot-Tagging-of-Natural-Language-Utterances",
        "muiIcon": "Abc",
        "description": "- Designed and implemented a BiLSTM with attention mechanism for slot tagging tasks, achieving 75.37% accuracy and F1 scores of 0.950 (sklearn) and 0.839 (seqeval).\n- Integrated pre-trained GloVe embeddings and created custom embedding matrices for semantic token representations.\n- Tuned hyperparameters, including dropout rates and learning rates, to improve model performance and generalization.\n- Evaluated token- and sequence-level metrics to capture complex token dependencies.",
        "tools": [
            "Python",
            "PyTorch",
            "GloVe",
            "Seqeval",
            "Scikit-learn"
        ]
    },
    {
        "title": "Transformer Language Model on Penn Treebank Dataset",
        "startDate": "November 2024",
        "endDate": "November 2024",
        "link": "https://github.com/iamsorenl/Language-Modeling-on-Penn-Treebank",
        "muiIcon": "Abc",
        "description": "- Developed a Transformer Encoder model for autoregressive language modeling on the Penn Treebank dataset, reducing test perplexity from 83.35 to 39.11.\n- Optimized model architecture with sinusoidal positional encoding and multi-head attention, improving generalization on small datasets.\n- Conducted extensive hyperparameter tuning to balance model complexity and training efficiency.\n- Evaluated predictive accuracy across training, validation, and test sets, validating the model's scalability for constrained datasets.",
        "tools": [
            "Python",
            "PyTorch",
            "Numpy",
            "Pandas",
            "Scikit-learn"
        ]
    },
    {
        "title": "Feature Engineering and Sentiment Analysis",
        "startDate": "October 2024",
        "endDate": "October 2024",
        "link": "https://github.com/iamsorenl/Feature-Engineering-and-Sentiment-Analysis",
        "muiIcon": "Abc",
        "description": "- Built a text classification pipeline for Amazon and IMDb datasets, exploring feature engineering techniques and comparing custom and scikit-learn models.\n- Implemented custom Naive Bayes, SVM, Decision Tree, and Logistic Regression classifiers with Laplacian smoothing and N-gram features.\n- Achieved 89% accuracy with SVM + TF-IDF on Amazon reviews and 87% validation accuracy with Logistic Regression + Bigrams on IMDb reviews.\n- Leveraged advanced metrics like binary and macro F1-scores to evaluate model performance.",
        "tools": [
            "Python",
            "Scikit-learn",
            "Numpy",
            "Matplotlib",
            "Bag-of-Words",
            "TF-IDF"
        ]
    },
    {
        "title": "Relation Extraction from Natural Language using PyTorch",
        "startDate": "October 2024",
        "endDate": "October 2024",
        "link": "https://github.com/iamsorenl/Relation-Extraction-from-Natural-Language-using-PyTorch",
        "muiIcon": "Abc",
        "description": "- Developed a Multi-Layer Perceptron (MLP) model to extract core relations from natural language utterances using the Freebase schema.\n- Utilized static embeddings from spaCy and trained the model with K-fold cross-validation for robust performance across folds.\n- Applied MultiLabelSoftMarginLoss to handle multi-label predictions effectively, achieving an average F1-score of ~0.91 and accuracy of ~0.86.\n- Optimized the architecture with batch normalization and dropout layers, demonstrating the model's utility in knowledge graph enrichment and conversational AI.",
        "tools": [
            "Python",
            "PyTorch",
            "SpaCy",
            "MultiLabelSoftMarginLoss",
            "K-fold Cross-Validation"
        ]
    }
]
